# Tree Transformer for Visual Language Navigation

## 1. Introduction

The aim of this project is to incorporate syntactic information into VLN (Visual Language Navigation) networks to improve cross modality alignment and decision making quality.

Tree transformer proposed to impose `hierarchical constraints` and `constituent attention` in transformers, so that low layers only have short-ranges attention, which gradually merge into long-ranged attention in high layers. This is to mimic the tree structures that we usually obtain when we parse a sentence.

**Topics:** _Visual Language Navigation (VLN)_, _Embodied AI_, _BERT_, _Tree Transformer_, _Natural Language Processing_, _Robotics_

**Skills:** _Pytorch_, _Python_, _Deep Neural Networks_, _Jupyter Lab_, _Colab_

## 2. Results

## 3. Acknowledgement

We acknowledge the use of codes from [SyntaxVLN](https://github.com/jialuli-luka/SyntaxVLN), [Recurrent VLN Bert](https://github.com/YicongHong/Recurrent-VLN-BERT), and [Tree Transformer](https://github.com/yaushian/Tree-Transformer).
